---
title: methods full draft 
---

Much research on browsing is conducted in physical libraries and then transposed onto digital settings. There are many good reasons for this, however, it does limit the extent to which the unique capabilities of digital technologies can be discussed, examined, or developed. Features of the digital knowledge creation cycle such as the browsing potential in hypertext-like systems are overlooked by these traditions as there is no physical world alternative to such behavior. This research proposes text analysis platforms as a potential environment for such investigation and lays a base of understanding for future researchers to build upon.

This project applied current browsing theory to the case study of text/corpus analysis systems/dashboards. These technologies embody a kind of browsing that is unique to the digital sphere. Among their uses is enabling the user to explore the text in a non-linear fashion, looking through linked segments of text, showing frequently mentioned words or phrases, and allowing the user to transfer text into visual data such as graphs and diagrams (Ramsay, 2014).

Twenty self-described 'discovery' text analysis tools make up the case studies that were used in this research. The primary methodology was the coding and analysis of numerous documents that surround these tools to analyze how their design anticipates users' browsing/ investigative/ and discovery behavior using them. These documents were then coded and analyzed looking at current browsing theory to examine if the anticipated behavior encoded into the tools is analogous to that described in the theory developed to understand physical libraries.

To this end, the research question was broken down into three sub-questions:

RQ: **How does the design of scholarly text analytic tools anticipate the information finding and processing behaviour of those who use them?**

1.  _Who_ designs scholarly text analysis tools and _what_ are the anticipated uses of these tools ?
2.  What kinds of exploratory/ browsing behavior is expected of users ?
3.  What 'bridges' are absent/implicit/invisible in these infrastructures as represented in the data ?

(1) - explicitly stated design & use expectation - This question draws attention to the creation of text analysis tools as interactions between multiple actors working within the constraints of technology and budget. In practice, this question required the least depth of analysis as the information can be pulled directly from the documentation. It serves to contextualize the findings of the rest of the analysis by giving information about the disciplinary field the tool originated from and the traditions associated with that field. \*\* saldana magnitude coding reference here ?

(2) - implicitly stated design & use expectations - This question examines the expectations for user browsing behavior that are implied by the documents. For example, help texts generally answer an assumed question or problem that is faced by the user. Furthermore, an aggregate of these implied problems gives a good idea of the user's anticipated behavior whilst working within the machine environment. Questions (1) and (2) were separated because it was considered of relevancy to the third question if the explicit expectation and the implicit expectations matched up or not. \*\* another prior reference here ?

(3) - what is not present in these documents / what labor is required in the use of these technologies that is not discussed in the documents - Within any technological design, there are biases specific to those who design and intend on using them. The purpose of this question is to "surface \[the\] invisible work" (Star, 1999) expected of those using framework and implied by the infrastructure that this framework creates. Document analysis is particularly advantageous as a methodology in this respect as it has historically been used to identify dominant narratives and biases (Prior, 2004).

These questions were explored via the close examination and analysis of the documentation that is available around the text analysis technologies that this study focuses upon including instructional documents that are integrated into the technology; white papers and research papers that are published alongside releases and updates of the technology; blogs detailing the development of the technology and notes; and notes pulled from the subject's source code. The open-source nature of many of these projects increased the availability of these resources. The further detailing of this process is the subject of the next section.

### Method in-depth

Document analysis was the primary methodology employed by this study because it allows an 'archeological examination of these tools. Most of these tools are the product of teams of people working together often in an iterative manner, this means that single voices from that team may not accurately have been able to report on the functions and implications of the tool. The documents used in this analysis provided the researcher with many angles from which to re-construct these functions and implications of these technologies as they stood at the time of data collection.

Although using document analysis as the primary methodology is unusual in social science, it is not unprecedented \*\* Goodall quotes. Furthermore, in this situation, it was felt that what was important to understand was the actual functionality and opinions embedded into the tools rather than the opinions of those who made them. This section will report on the details of this methodology first considering the approach to data gathering, the then texts themselves, and following on with the approach to data processing.

Data gathering

I used the [TAPoR (Text Analysis Portal for Research)](http://tapor.ca/home) collection of text mining and concentrated on those tagged with the description 'discovering'. This yielded 138 results. Of these, I excluded 14 because they were classified as 'archived', 1 because it was straight code rather than a tool or interface. Of the remaining 123, 30 were open-source, and still in active development. Of these 11 self-identified specifically as corpus analysis tools and a further 9 were included as they allowed for exploration within a single text. This lead to an initial number of 20 case studies. Among the text analysis tools excluded from this study were tools that specialized in scraping and analyzing social media posts and tools that organized large datasets and facilitated browsing between texts rather than within them.

For each of the texts identified I pulled the documentation/ help text to analyze separately in nViVo and made screenshots of the UI to analyses the visual layout of the manuals as well. I also pulled screenshots of the tools themselves for visual design analysis.

Kinds of documents

The cornerstone of this dataset is the instructional texts that are pulled from each technology. Most often these are supplementary web pages that are linked from different points of the tool. These documents were selected because they are publically available and because they seemed likely to provide the information needed to answer the questions proposed in this research. The instructional documents in particular were especially valued in this respect.

Although they may seem rather dry the instruction manuals were key data in this research. A number of questions must be asked when designing tools such as these: who is the prospective user; what level of technical understanding do they come to this tool with; what problems are they likely to come across? The resulting help texts that accompany such a tool are indicative of the tool creators' answers to these questions. The degree of specialist language is used in the document, the level of technical literacy that is assumed of the users. These factors all influence the kinds of problems that are the focus of help texts. Furthermore, these help texts are the main point in which the tool creators interact explicitly with the user rather than through their technology.

Data processing

The documents in this study are not considered to be static entities but rather are entities situated within the context of the documents surrounding them (Prior, 2004). This is why, when available, this study used multiple documents pertaining to a single technology, to help situate the individual documents in the process by which they were created. In a similar vein, the coding of this data placed emphasis on the distinction between the production, consumption, and content of this data (Prior, 2004) in order to capture as much nuance as possible for the analysis.

It is important to consider how a document is put together. A physical instruction manual may have a single author and be published at a single point however most of these online manuals are continually updated by different authors (Prior, 2004). The process through which users find, access, and navigate these documents will also be considered. Using separate passes to code these documents from each perspective.

Following the findings of a prior experimental coding pass on a subsection of the dataset, the coding procedure was decided. An initial descriptive code of all materials over several passes, considering the distinction between the production, consumption, and content of this data was conducted, following this and using the conceptual framework for guidance an initial code list was created and run through several more passes of the data updating the code list where necessary. The codes and memos that result from this process will be grouped and compared to theory in the analysis portion of the essay.

In the initial coding process it was found to be useful to use a degree of magnitude coding through out the process specifically in the instruction documents. These documents are quite dry and part of the interest in them is the question of who they are written for - what level of technical competency is expected, what problems are anticipated, what queries is this prospective user expected to ask, how are they formulated etc. Many of these codes need to be done manually.

Analysis

The analysis for this work was done by applying the conceptual framework and the literature review to the findings from the document analysis.

summing up

This section has sought to lay out the methodology of this research covering the primary methodology, the methodological fit, the philosophy supporting the methodology, and the various processes by which that methodology was enacted. The ethical considerations for this project were very few due to the open-source nature of the technology themselves, publicly available documents, and the impersonal nature of the inquiry.

\*\* to add

-   methodological standpoint - ie ethnography - sources from Rebecca
-   explicit philosophical standpoint - interpretive - find someone who has done this well to see style
-   more referencing would be good