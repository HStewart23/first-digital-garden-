---
title: research questions and hypothesis
---

past question :  'How do natural language processing tools for the exploration of digital texts affect or challenge previous conceptions of browsing?'

past hypothesis (methods paper):  'The hypothesis of this research is that the changing nature of data in our society (Zuboff, 2019) has likely influenced the normative concepts that describe our relationships to that data, concepts such as browsing. It is therefore the purpose of this research to consider prominent browsing conceptualisation frameworks from the 1990s and early 2000s and compare them to modern technology within the realm of digital humanities in order to consider if they still fit and are as unilaterally relevant as their usage would suggest.''

current question: 
Considering the prevelence of large corpus analytics tools in the digital humantities and human-computer interaction studies **how does the design of large corpus analytics tools anticiptate the information finding and processing behaviour of those who use them ?** 

question I'm not sure I can ask: 
How will this effect the kind of knowlege that can be produced from these technologies 

---

Draft text :

**A problem of platforms**

This research addresses the lack of scholarship surrounding the widespread use of corpus analysis technology as a means of information exploration in texts. The focus of this study is on the intentions and assumptions that are intentionally or unintentionally encoded into these tools as this is essential information to understand before a question about their usage and users' experiences with them. To this end, the research question was broken down into three sub-questions:

RQ: **How does the design of scholarly text analytic tools anticipate the information finding and processing behaviour of those who use them?**

1.  _Who_ designs scholarly text analysis tools and _what_ are the anticipated uses of these tools ?
2.  What kinds of exploratory/ browsing behaviour is expected of users ?
3.  What 'bridges' are absent/implicit/invisible in these infrastructures as represented in the data

(1) - explicitly stated design & use expectation - Draws attention to the creation of text analysis tools as interactions between multiple actors working within the constraints of technology and budget. In practice, this question required the least depth of analysis as the information can be pulled directly from the documentation. It serves to contextualize the findings of the rest of the analysis by giving information about the disciplinary field the tool originated from and the traditions associated with that field.

(2) - implicitly stated design & use expectations - This question examines the expectations for user browsing behavior that are implied by the documents. For example, help texts generally answer an assumed question or problem that is faced by the user. Furthermore, an aggregate of these implied problems gives a good idea of the user's anticipated behavior whilst working within the machine environment. Questions (1) and (2) were separated because it was considered of relevancy to the third question if the explicit expectation and the implicit expectations matched up or not.

(3) - what is not present in these documents / what labour is required in the use of these technologies that is not discussed in the documents - Within any technological design, there are biases specific to those who design and intend on using them. Both prior and star note one of the advantages of document analysis is the provision of clear inclusion/ exclusion decisions. This question's purpose is the leverage this methodology's ability to consider the accessibility and potential consequences of building a knowledge infrastructure in this way.

These questions were explored via the close examination and analysis of the copious documentation that is available around open-source tech projects such as these including: instructional documents that are integrated into the technology; white papers and research papers that are published alongside releases and updates of the technology; blogs detailing the development of the technology and notes; and notes pulled from the subject's source code. The further detailing of this process is the subject of the next section.

